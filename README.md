ğŸ“š RAG Chatbot â€” FAISS + Gemini 1.5 Flash

A simple, modular Retrieval-Augmented Generation (RAG) chatbot built using Python, FAISS, Sentence Transformers, and Gemini-1.5-Flash.

ğŸš€ Overview

This project demonstrates a beginner-friendly yet powerful RAG (Retrieval-Augmented Generation) chatbot.
It loads your documents, chunks them, embeds them, stores them in a FAISS vector database, retrieves relevant information based on a query, and uses Gemini-1.5-Flash to generate accurate answers.

A simple Streamlit UI is included for an easy interactive chat experience.

âœ¨ Features

ğŸ” Semantic search using FAISS

ğŸ§  Gemini-1.5-Flash for fast, high-quality LLM responses

ğŸ“„ ETL pipeline for extracting, transforming, and loading documents

ğŸ§© Fully modular codebase â€” perfect for learning

ğŸ’¬ Streamlit web UI

âš¡ Lightweight and beginner-friendly


ğŸ› ï¸ Installation
1. Create Conda Environment
conda create -n ragbot python=3.10 -y
conda activate ragbot

2. Install Dependencies
pip install faiss-cpu sentence-transformers langchain google-generativeai python-dotenv streamlit

ğŸ”‘ Setup Your API Key

Create a .env file in the project root:

GEMINI_API_KEY=your_api_key_here

ğŸ“„ Add Your Documents

Place your .txt files inside the data/ folder.

Example:

data/
â””â”€â”€ sample.txt

â–¶ï¸ Running the Chatbot (Terminal Mode)
python main.py

ğŸ’» Running the Web UI (Streamlit)
streamlit run app.py


This launches an interactive browser-based UI.

ğŸ§  How It Works (RAG Pipeline)

Extract â†’ Loads .txt files

Transform â†’ Cleans + chunks text into small pieces

Embed â†’ Converts chunks into vector embeddings

Store â†’ Saves vectors inside a FAISS index

Retrieve â†’ Finds top relevant chunks for a query

Generate â†’ Gemini-1.5-Flash uses retrieved context to answer

ğŸ§© Technologies Used

Python

FAISS â€” Vector similarity search

Sentence Transformers â€” Generating embeddings

Gemini 1.5 Flash â€” Large Language Model

Streamlit â€” Web UI

dotenv â€” Environment variable support

